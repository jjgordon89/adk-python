# ============================================================================
# SafetyCulture ADK Agent - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your actual values
# DO NOT commit .env to version control


# ============================================================================
# SafetyCulture API Configuration
# ============================================================================
# Required for all SafetyCulture operations
SAFETYCULTURE_API_TOKEN=your_safetyculture_api_token_here
# Get your API token from: https://app.safetyculture.com/account/api-tokens


# ============================================================================
# Google Cloud Configuration (Required for Gemini)
# ============================================================================
# Required for Gemini native implementation
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_REGION=us-central1

# Optional: Path to service account key file
# If not set, uses Application Default Credentials
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json


# ============================================================================
# Llama via Vertex AI Model-as-a-Service (Optional)
# ============================================================================
# Enable Llama provider in models.yaml to use these
# Uses same Google Cloud credentials as Gemini
# VERTEX_AI_LOCATION=us-central1


# ============================================================================
# Nvidia API Configuration (Optional)
# ============================================================================
# Required if using Nvidia provider
# Get your API key from: https://build.nvidia.com/
# NVIDIA_API_KEY=your_nvidia_api_key_here


# ============================================================================
# Ollama Configuration (Optional - Local Development)
# ============================================================================
# Required if using Ollama for local model inference
# Default: http://localhost:11434
# OLLAMA_BASE_URL=http://localhost:11434

# Installation instructions for Ollama:
# 1. Download from: https://ollama.ai/download
# 2. Install Ollama
# 3. Pull models: ollama pull llama3
# 4. Verify: ollama list


# ============================================================================
# LiteLLM Configuration (Optional)
# ============================================================================
# Additional LiteLLM settings for non-Gemini providers
# LITELLM_LOG=DEBUG  # Enable verbose logging for debugging
# LITELLM_DROP_PARAMS=true  # Drop unsupported params instead of erroring


# ============================================================================
# ADK Configuration (Optional)
# ============================================================================
# Agent behavior configuration
# ADK_LOG_LEVEL=INFO
# ADK_MAX_RETRIES=3
# ADK_TIMEOUT=300


# ============================================================================
# Development Settings (Optional)
# ============================================================================
# Use local model configuration override
# MODEL_CONFIG_PATH=./safetyculture_agent/config/models.local.yaml

# Enable debug mode for detailed logging
# DEBUG=true